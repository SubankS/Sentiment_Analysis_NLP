{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn import model_selection, naive_bayes \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the positive reviews into the dataframe df_pos\n",
    "data_pos = []\n",
    "all_files = os.listdir(\"positive_reviews/\")\n",
    "for x in all_files:\n",
    "    path = os.path.join(\"positive_reviews\",x)\n",
    "    with open(path,\"r\",encoding=\"utf8\") as f:\n",
    "        a = []\n",
    "        a.append(str(f.readline()))\n",
    "        a.append(\"1\")\n",
    "        data_pos.append(a)\n",
    "df_pos = pd.DataFrame(data_pos, columns = ['text', 'label'])\n",
    "\n",
    "#appending the negative reviews into the dataframe df_neg\n",
    "data_neg = []\n",
    "all_files = os.listdir(\"negative_reviews/\")\n",
    "for x in all_files:\n",
    "    path = os.path.join(\"negative_reviews\",x)\n",
    "    with open(path,\"r\",encoding=\"utf8\") as f:\n",
    "        a = []\n",
    "        a.append(str(f.readline()))\n",
    "        a.append(\"0\")\n",
    "        data_neg.append(a)\n",
    "df_neg = pd.DataFrame(data_neg, columns = ['text','label'])\n",
    "\n",
    "df = pd.concat([df_pos,df_neg]).sort_index(kind='merge') #merging df_pos and df_neg into df\n",
    "df['index'] = np.arange(len(df)) #creating a new index column\n",
    "df= df.set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove blank rows if any.\n",
    "df['text'].dropna(inplace=True)\n",
    "#Change all the text to lower case to avoid ambiguity\n",
    "df['text_mod'] = [entry.lower() for entry in df['text']]\n",
    "#Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "df['text_mod']= [word_tokenize(entry) for entry in df['text_mod']]\n",
    "#Remove Stop words, Numeric and perfom Word Stemming/Lemmatization \n",
    "#Stopwords are commonly used words like \"to,the,in\" which can be removed from Corpus and doesn't affect the accuracy much.\n",
    "#Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as \n",
    "#a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar \n",
    "#meaning to one word.Like \"Saying,said,says\" reduced to \"say\"\n",
    "#POS_tag determines the Part of Speech to which the word belong.\n",
    "tag_map = defaultdict(lambda : wn.NOUN) #default is NOUN\n",
    "tag_map['J'] = wn.ADJ #Adjective\n",
    "tag_map['V'] = wn.VERB #Verb\n",
    "tag_map['R'] = wn.ADV #Adverb\n",
    "for index,entry in enumerate(df['text_mod']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) / Verb(V) / Adjective(J) / Adverb(R)\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_fin = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_fin)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    df.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['text_final'],df['label'],test_size=0.2,random_state=42)\n",
    "target_names = ['pos', 'neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logistic Regression Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.90      0.87      0.89      2492\n",
      "         neg       0.88      0.90      0.89      2508\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pipeline - Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be\n",
    "# ‘transforms’,that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "# Count Vectorizer - Convert a collection of text documents to a matrix of token counts. This implementation produces a \n",
    "# sparse representation of the counts\n",
    "# Tfidf : Term Frequency - Inverse Document Frequency\n",
    "\n",
    "# Logistic Regression\n",
    "text_clf_logreg = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf_logreg', LogisticRegression(random_state=42,max_iter=200)),])\n",
    "text_clf_logreg = text_clf_logreg.fit(Train_X, Train_Y)\n",
    "predicted_logreg = text_clf_logreg.predict(Test_X)\n",
    "print(\"---Logistic Regression Classification Report---\")\n",
    "print(classification_report(Test_Y, predicted_logreg, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SGD Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.91      0.77      0.84      2492\n",
      "         neg       0.80      0.93      0.86      2508\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.86      0.85      0.85      5000\n",
      "weighted avg       0.86      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_sgd = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-sgd', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=100, random_state=42)),])\n",
    "text_clf_sgd = text_clf_sgd.fit(Train_X, Train_Y)\n",
    "predicted_sgd = text_clf_sgd.predict(Test_X)\n",
    "print(\"---SGD Classification Report---\")\n",
    "print(classification_report(Test_Y, predicted_sgd, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Naive Bayes Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.85      0.88      0.87      2492\n",
      "         neg       0.87      0.85      0.86      2508\n",
      "\n",
      "    accuracy                           0.86      5000\n",
      "   macro avg       0.86      0.86      0.86      5000\n",
      "weighted avg       0.86      0.86      0.86      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_nb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n",
    "text_clf_nb = text_clf_nb.fit(Train_X, Train_Y)\n",
    "predicted_nb = text_clf_nb.predict(Test_X)\n",
    "print(\"---Naive Bayes Classification Report---\")\n",
    "print(classification_report(Test_Y, predicted_nb, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SVM Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.89      0.88      0.89      2492\n",
      "         neg       0.88      0.90      0.89      2508\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-svm', SVC(C=1.0, kernel='linear', degree=3, gamma='auto')),])\n",
    "text_clf_svm = text_clf_svm.fit(Train_X, Train_Y)\n",
    "predicted_svm = text_clf_svm.predict(Test_X)\n",
    "print(\"---SVM Classification Report---\")\n",
    "print(classification_report(Test_Y, predicted_svm, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Decision Tree Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.72      0.71      0.71      2492\n",
      "         neg       0.71      0.72      0.72      2508\n",
      "\n",
      "    accuracy                           0.71      5000\n",
      "   macro avg       0.71      0.71      0.71      5000\n",
      "weighted avg       0.71      0.71      0.71      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_dt = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-dt', DecisionTreeClassifier()), ])\n",
    "text_clf_dt = text_clf_dt.fit(Train_X, Train_Y)\n",
    "predict_dt = text_clf_dt.predict(Test_X)\n",
    "print(\"---Decision Tree Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_dt, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ExtraTreeClassifier Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.64      0.63      0.64      2492\n",
      "         neg       0.64      0.65      0.64      2508\n",
      "\n",
      "    accuracy                           0.64      5000\n",
      "   macro avg       0.64      0.64      0.64      5000\n",
      "weighted avg       0.64      0.64      0.64      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_etc = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-etc', ExtraTreeClassifier()), ])\n",
    "text_clf_etc = text_clf_etc.fit(Train_X, Train_Y)\n",
    "predict_etc = text_clf_etc.predict(Test_X)\n",
    "print(\"---Extra Tree Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_etc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Multi Layer Perceptron Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.88      0.87      0.88      2492\n",
      "         neg       0.87      0.88      0.88      2508\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_mlp = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-mlp', MLPClassifier()), ])\n",
    "text_clf_mlp = text_clf_mlp.fit(Train_X, Train_Y)\n",
    "predict_mlp = text_clf_mlp.predict(Test_X)\n",
    "print(\"---Multi Layer Perceptron Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_mlp, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Ridge Classifier Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.89      0.88      0.89      2492\n",
      "         neg       0.88      0.89      0.89      2508\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_rc = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-rc', RidgeClassifier()), ])\n",
    "text_clf_rc = text_clf_rc.fit(Train_X, Train_Y)\n",
    "predict_rc = text_clf_rc.predict(Test_X)\n",
    "print(\"---Ridge Classifier Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_rc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---AdaBoost Classifier Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.84      0.80      0.82      2492\n",
      "         neg       0.81      0.85      0.83      2508\n",
      "\n",
      "    accuracy                           0.82      5000\n",
      "   macro avg       0.83      0.82      0.82      5000\n",
      "weighted avg       0.83      0.82      0.82      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_adb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-adb', AdaBoostClassifier(n_estimators=100)), ])\n",
    "text_clf_adb = text_clf_adb.fit(Train_X, Train_Y)\n",
    "predict_adb = text_clf_adb.predict(Test_X)\n",
    "print(\"---AdaBoost Classifier Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_adb, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GradientBoost Classifier Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.83      0.74      0.78      2492\n",
      "         neg       0.77      0.85      0.81      2508\n",
      "\n",
      "    accuracy                           0.80      5000\n",
      "   macro avg       0.80      0.80      0.80      5000\n",
      "weighted avg       0.80      0.80      0.80      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_gb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-gb', GradientBoostingClassifier()), ])\n",
    "text_clf_gb = text_clf_gb.fit(Train_X, Train_Y)\n",
    "predict_gb = text_clf_gb.predict(Test_X)\n",
    "print(\"---GradientBoost Classifier Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_gb, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Random Forest Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         pos       0.76      0.73      0.75      2492\n",
      "         neg       0.74      0.77      0.76      2508\n",
      "\n",
      "    accuracy                           0.75      5000\n",
      "   macro avg       0.75      0.75      0.75      5000\n",
      "weighted avg       0.75      0.75      0.75      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf_rf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-rf', RandomForestClassifier(max_depth=2)), ])\n",
    "text_clf_rf = text_clf_rf.fit(Train_X, Train_Y)\n",
    "predict_rf = text_clf_rf.predict(Test_X)\n",
    "print(\"---Random Forest Classification Report---\")\n",
    "print(classification_report(Test_Y, predict_rf, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
